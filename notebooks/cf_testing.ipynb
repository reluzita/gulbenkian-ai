{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.10 64-bit ('py36': conda)",
   "metadata": {
    "interpreter": {
     "hash": "735faef179f5cb085525f3faaa83caf951f2a05dfe70ead795f24f805eb4c248"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import SVD, SVDpp, SlopeOne, NMF, NormalPredictor, KNNBaseline, KNNBasic, KNNWithMeans, KNNWithZScore, BaselineOnly, CoClustering\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import accuracy\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('C:\\\\Users\\\\ineso\\\\FEUP-3ano\\\\gulbenkian-ai\\\\data\\\\vegas-restaurants\\\\vegas_review_reduced.pickle')\n",
    "df = df.sample(n=20000, random_state=1)\n",
    "\n",
    "trainset, testset = train_test_split(df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_df = pd.read_pickle('C:\\\\Users\\\\ineso\\\\FEUP-3ano\\\\gulbenkian-ai\\\\data\\\\vegas-restaurants\\\\restaurant_in_vegas.pickle')\n",
    "rest_df = rest_df[['business_id', 'categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_trainset = pd.DataFrame(columns=['user_id', 'category', 'stars'])\n",
    "temp = trainset.merge(rest_df, how='inner', on='business_id')\n",
    "i=0\n",
    "for index, row in temp.iterrows():\n",
    "    categories = row['categories'].split(\", \")\n",
    "    for c in categories:\n",
    "        cat_trainset.loc[i] = [row['user_id'], c, row['stars']]\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = cat_trainset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = testset.merge(rest_df, how='inner', on='business_id')\n",
    "for index, row in temp2.iterrows():\n",
    "    categories = row['categories'].split(\", \")\n",
    "    for c in categories:\n",
    "        cat_data.loc[i] = [row['user_id'], c, row['stars']]\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = pd.read_pickle('C:\\\\Users\\\\ineso\\\\FEUP-3ano\\\\gulbenkian-ai\\\\data\\\\categories_testing.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_data = cat_data.sample(n=20000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                 test_rmse  test_mae  fit_time  test_time\n",
       "Algorithm                                                \n",
       "SlopeOne          0.984850  0.596110  0.119629   0.031467\n",
       "KNNWithZScore     0.988068  0.611777  4.660350   1.463275\n",
       "KNNWithMeans      0.989905  0.601487  4.322998   1.325323\n",
       "CoClustering      0.992184  0.634548  1.620576   0.025207\n",
       "NMF               1.050719  0.776954  1.893413   0.028193\n",
       "KNNBaseline       1.168486  0.884412  4.167697   1.445557\n",
       "KNNBasic          1.205626  0.906299  4.063173   1.843688\n",
       "SVDpp             1.251228  1.018078  1.920397   0.050510\n",
       "SVD               1.289752  1.058490  1.023424   0.335473\n",
       "BaselineOnly      1.315779  1.090573  0.055112   0.024621\n",
       "NormalPredictor   1.794460  1.413340  0.028600   0.028808"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>test_rmse</th>\n      <th>test_mae</th>\n      <th>fit_time</th>\n      <th>test_time</th>\n    </tr>\n    <tr>\n      <th>Algorithm</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>SlopeOne</th>\n      <td>0.984850</td>\n      <td>0.596110</td>\n      <td>0.119629</td>\n      <td>0.031467</td>\n    </tr>\n    <tr>\n      <th>KNNWithZScore</th>\n      <td>0.988068</td>\n      <td>0.611777</td>\n      <td>4.660350</td>\n      <td>1.463275</td>\n    </tr>\n    <tr>\n      <th>KNNWithMeans</th>\n      <td>0.989905</td>\n      <td>0.601487</td>\n      <td>4.322998</td>\n      <td>1.325323</td>\n    </tr>\n    <tr>\n      <th>CoClustering</th>\n      <td>0.992184</td>\n      <td>0.634548</td>\n      <td>1.620576</td>\n      <td>0.025207</td>\n    </tr>\n    <tr>\n      <th>NMF</th>\n      <td>1.050719</td>\n      <td>0.776954</td>\n      <td>1.893413</td>\n      <td>0.028193</td>\n    </tr>\n    <tr>\n      <th>KNNBaseline</th>\n      <td>1.168486</td>\n      <td>0.884412</td>\n      <td>4.167697</td>\n      <td>1.445557</td>\n    </tr>\n    <tr>\n      <th>KNNBasic</th>\n      <td>1.205626</td>\n      <td>0.906299</td>\n      <td>4.063173</td>\n      <td>1.843688</td>\n    </tr>\n    <tr>\n      <th>SVDpp</th>\n      <td>1.251228</td>\n      <td>1.018078</td>\n      <td>1.920397</td>\n      <td>0.050510</td>\n    </tr>\n    <tr>\n      <th>SVD</th>\n      <td>1.289752</td>\n      <td>1.058490</td>\n      <td>1.023424</td>\n      <td>0.335473</td>\n    </tr>\n    <tr>\n      <th>BaselineOnly</th>\n      <td>1.315779</td>\n      <td>1.090573</td>\n      <td>0.055112</td>\n      <td>0.024621</td>\n    </tr>\n    <tr>\n      <th>NormalPredictor</th>\n      <td>1.794460</td>\n      <td>1.413340</td>\n      <td>0.028600</td>\n      <td>0.028808</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "cat_data2 = Dataset.load_from_df(cat_data[['user_id', 'category', 'stars']], reader)\n",
    "benchmark = []\n",
    "# Iterate over all algorithms\n",
    "for algorithm in [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]:\n",
    "    # Perform cross validation\n",
    "    results = cross_validate(algorithm, cat_data2, measures=['RMSE', 'MAE'], cv=5, verbose=False)\n",
    "    \n",
    "    # Get results & append algorithm name\n",
    "    tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n",
    "    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n",
    "    benchmark.append(tmp)\n",
    "    \n",
    "pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train_data = Dataset.load_from_df(cat_trainset[['user_id', 'category', 'stars']], reader)\n",
    "cat_train_set = cat_train_data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_trainset.to_pickle('C:\\\\Users\\\\ineso\\\\FEUP-3ano\\\\gulbenkian-ai\\\\data\\\\categories_testing_train.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_test_set = testset.merge(rest_df, how='inner', on='business_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test_rmse 1.452314676968907\n"
     ]
    }
   ],
   "source": [
    "# [SVD(), SVDpp(), SlopeOne(), NMF(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), BaselineOnly(), CoClustering()]\n",
    "algorithm = CoClustering()\n",
    "algorithm.fit(cat_train_set)\n",
    "   \n",
    "sqr_errors = []\n",
    "for index, row in cat_test_set.iterrows():\n",
    "    categories = row['categories'].split(\", \")\n",
    "    predictions = []\n",
    "    for c in categories:\n",
    "        predictions.append(algorithm.predict(row['user_id'], c, r_ui=row['stars']).est)\n",
    "    predicted_rating = sum(predictions)/len(predictions)\n",
    "    sqr_errors.append((row['stars'] - predicted_rating)**2)\n",
    "\n",
    "print('test_rmse', math.sqrt(sum(sqr_errors)/len(sqr_errors)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(1, 5))\n",
    "train_data = Dataset.load_from_df(trainset[['user_id', 'business_id', 'stars']], reader)\n",
    "train_set = train_data.build_full_trainset()\n",
    "\n",
    "test_data = Dataset.load_from_df(testset[['user_id', 'business_id', 'stars']], reader)\n",
    "test_set = test_data.build_full_trainset()\n",
    "test_set = test_set.build_testset()"
   ]
  }
 ]
}